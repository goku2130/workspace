{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbOvkKpuac3UlnVNJoLwpc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goku2130/workspace/blob/master/Parity%20XOR%20using%20LSTM(Open%20AI%20Challenge).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfQ3g8mHZTlt"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "MAX_SEQ_LEN = 100\n",
        "BATCH = 1024\n",
        "IS_FIXED_LEN = False\n",
        "NUM_SAMPLES = 100000\n",
        "POST_PADDING = False\n",
        "\n",
        "def generator_xor(is_fixed_len = True):\n",
        "    \"\"\"\n",
        "    Generates a dataset of a sequence of 1's and 0's using numpy functions and also its parity. It provides a \n",
        "    capability to have a fixed length sequence or variable length\n",
        "    @param is_fixed_len(Bool): Whether length is fixed or not\n",
        "    @return: a tuple of two numpy arrays: output parity\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "\n",
        "    while i < NUM_SAMPLES:\n",
        "\n",
        "        if is_fixed_len:\n",
        "\n",
        "            max_seq_len = MAX_SEQ_LEN\n",
        "\n",
        "        else:\n",
        "           \n",
        "            max_seq_len = np.random.randint(1, MAX_SEQ_LEN)\n",
        "\n",
        "        series = np.random.randint(low=0, high=2, size=(max_seq_len))\n",
        "        output = np.array([1 - np.sum(series) % 2 , np.sum(series) % 2])\n",
        "        series = np.reshape(series,(max_seq_len, 1))\n",
        "        yield output, series\n",
        "        i += 1\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoRqwiG8iqJ9",
        "outputId": "12b010be-7735-4936-ed47-92eccd0d0c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for op,seq in generator_xor(is_fixed_len = IS_FIXED_LEN):\n",
        "  print('Sample output: XOR =>{} \\n Sequence => {}'.format(op, seq))\n",
        "  break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample output: XOR =>[0 1] \n",
            " Sequence => [[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX-guQ4xZqJN",
        "outputId": "e8d4af98-625b-41be-fd1e-b3f3fd82de2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Input(shape=(None, 1)))\n",
        "model.add(tf.keras.layers.Masking(mask_value=-10, input_shape=(MAX_SEQ_LEN, 1)))\n",
        "model.add(tf.keras.layers.LSTM(1, return_sequences= False, stateful=False,))\n",
        "model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy']\n",
        ")\n",
        "print(model.summary())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_1 (Masking)          (None, None, 1)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 1)                 12        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 4         \n",
            "=================================================================\n",
            "Total params: 16\n",
            "Trainable params: 16\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChLUxYIWwVfi"
      },
      "source": [
        "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
        "    \"\"\"\n",
        "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
        "    rounded up to two decimal places.\n",
        "    @param dataset: the input dataset to split.\n",
        "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
        "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
        "    \"\"\"\n",
        "\n",
        "    validation_data_percent = round(validation_data_fraction * 100)\n",
        "    if not (0 <= validation_data_percent <= 100):\n",
        "        raise ValueError(\"validation data fraction must be âˆˆ [0,1]\")\n",
        "\n",
        "    dataset = dataset.enumerate()\n",
        "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
        "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
        "\n",
        "    # remove enumeration\n",
        "    train_dataset = train_dataset.map(lambda f, data: data)\n",
        "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
        "\n",
        "    return train_dataset, validation_dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuXrGZuK0p9S",
        "outputId": "add2817f-1593-403f-8fde-e95b855f4116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def xor_data_generator_wrapper(batch_size, train_flag = True):\n",
        "  data_series = tf.data.Dataset.from_generator(generator_xor, args=[IS_FIXED_LEN], \n",
        "                                                output_types=(tf.int32, tf.int32), \n",
        "                                                output_shapes=((2,), (None, 1)))\n",
        "\n",
        "  #train_series, val_series = split_dataset(data_series, 0.2)\n",
        "  \n",
        "  if train_flag:\n",
        "\n",
        "    train_series = data_series.padded_batch(batch_size = batch_size, padding_values=-10,\n",
        "                                            padded_shapes=([2,], [MAX_SEQ_LEN, 1]))\n",
        "    if POST_PADDING:\n",
        "\n",
        "      while True:\n",
        "          batch_output, batch_seq = next(iter(train_series))\n",
        "          yield batch_seq, batch_output \n",
        "\n",
        "    else:\n",
        "\n",
        "       while True:\n",
        "          batch_output, batch_seq = next(iter(train_series))\n",
        "          yield tf.reverse(batch_seq, [1]) , batch_output \n",
        "\n",
        "  \"\"\"else:\n",
        "\n",
        "    val_series = val_series.padded_batch(batch_size = batch_size, padding_values=-10,\n",
        "                                        padded_shapes=([2,], [MAX_SEQ_LEN, 1]))\n",
        "\n",
        "    while True:\n",
        "      batch_output, batch_seq = next(iter(val_series))\n",
        "      yield tf.reverse(batch_seq, [1]), batch_output \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "num_batches = int(NUM_SAMPLES/BATCH)\n",
        "train_data_generator = xor_data_generator_wrapper(batch_size=BATCH)\n",
        "model.fit_generator(generator=train_data_generator,\n",
        "                    steps_per_epoch=num_batches, epochs=10, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 30s 308ms/step - loss: 0.6932 - accuracy: 0.5009\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 28s 289ms/step - loss: 0.6932 - accuracy: 0.4998\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 29s 296ms/step - loss: 0.6932 - accuracy: 0.5025\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 29s 304ms/step - loss: 0.6932 - accuracy: 0.4973\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 29s 296ms/step - loss: 0.6931 - accuracy: 0.5055\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 29s 295ms/step - loss: 0.6932 - accuracy: 0.5038\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 29s 297ms/step - loss: 0.6918 - accuracy: 0.5155\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 29s 303ms/step - loss: 0.3324 - accuracy: 0.9527\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 31s 317ms/step - loss: 0.1242 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 29s 298ms/step - loss: 0.0803 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2012d62780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}